import tensorflow as tf
import matplotlib.pyplot as plt
X=[1,2,3]
Y=[1,2,3]

W=tf.placeholder(tf.float32)
#Our hypothesis for linear model X*W
hypothesis=X*W

#cost/loss function
cost=tf.reduce_mean(tf.square(hypothesis-Y))
#Launch the graph in a session
sess=tf.Session()

#Initializes global variables in the graph
sess.run(tf.global_variables_initializer())

#Variables for plotting cost function
#W와 cost의 값을 저장할 list를 만듬
W_val=[]
cost_val=[]
for i in range(-30,50):
    feed_W=i*0.1
    curr_cost,curr_W=sess.run([cost,W], feed_dict={W:feed_W})
    W_val.append(curr_W)
    cost_val.append(curr_cost)
    
#show the cost function
plt.plot(W_val, cost_val)
plt.show()











import tensorflow as tf
x_data = [1,2,3]
y_data = [1,2,3]

W=tf.Variable(tf.random_normal([1]), name='weight')
X=tf.placeholder(tf.float32)
Y=tf.placeholder(tf.float32)

#Our hypothesis for linear model X*W
hypothesis = X*W

#cost/loss function
cost=tf.reduce_sum(tf.square(hypothesis-Y))

#Minimize: Gradient Descent using derivative:
#W-=learning_rate*derivative
learning_rate=0.1
gradient=tf.reduce_mean((W*X-Y)*X)
descent=W-learning_rate*gradient
update=W.assign(descent)
#update를 실행만 시켜주면 위의 모든 과정일 일어나게됨

#Launch the graph in a session.
sess=tf.Session()

#Initializes global variables in the graph
sess.run(tf.global_variables_initializer())
for step in range(21):
    sess.run(update, feed_dict={X:x_data, Y:y_data})
    print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))

    
#step, cost, W 순으로 출력됨
#Minimize: Gradient Descent Magic
#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
#train=optimizer.minimize(cost)하면 자동으로 위의 과정 해줌








